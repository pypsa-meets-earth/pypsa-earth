{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OLD NOTEBOOK: SEE osm_pbf_power_data_extractor.py which does everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time\n",
    "\n",
    "sys.path.append(\"../scripts\")\n",
    "from osm_data_config import AFRICA_CC\n",
    "\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "# https://gitlab.com/dlr-ve-esy/esy-osmfilter/-/tree/master/\n",
    "from esy.osmfilter import osm_colors as CC\n",
    "from esy.osmfilter import run_filter, Node, Way, Relation\n",
    "from esy.osmfilter import export_geojson\n",
    "from esy.osmfilter import osm_info as osm_info\n",
    "from esy.osmfilter import osm_pickle as osm_pickle\n",
    "from contextlib import contextmanager\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, LineString\n",
    "import geoplot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# import logging\n",
    "# logging.basicConfig()\n",
    "# logger=logging.getLogger(__name__)\n",
    "# logger.setLevel(logging.INFO)\n",
    "# logger.setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Redundant code to create AFRICA_CC\n",
    "\n",
    "# from osm_data_config import CC\n",
    "# geofabrik_regions = [\"algeria\",\"angola\",\"benin\",\"botswana\",\"burkina_faso\",\"burundi\",\"cameroon\",\"canary_islands\",\"cape_verde\",\"central_african_republic\",\"chad\",\"comores\",\"congo_brazzaville\",\"congo_democratic_republic\",\"djibouti\",\"egypt\",\"equatorial_guinea\",\"eritrea\",\"ethiopia\",\"gabon\",\"ghana\",\"guinea_bissau\",\"guinea\",\"ivory_coast\",\"kenya\",\"lesotho\",\"liberia\",\"libya\",\"madagascar\",\"malawi\",\"mali\",\"mauritania\",\"mauritius\",\"morocco\",\"mozambique\",\"namibia\",\"niger\",\"nigeria\",\"rwanda\",\"saint_helena_ascension_and_tristan_da_cunha\",\"sao_tome_and_principe\",\"senegal_and_gambia\",\"seychelles\",\"sierra_leone\",\"somalia\",\"south_africa_and_lesotho\",\"south_africa\",\"south_sudan\",\"sudan\",\"swaziland\",\"tanzania\",\"togo\",\"tunisia\",\"uganda\",\"zambia\",\"zimbabwe\"]\n",
    "\n",
    "# for region in geofabrik_regions:\n",
    "#     try :\n",
    "#         print('\"'+list(CC.keys())[list(CC.values()).index(region.replace('_',' ').upper())]+'\":\"'+region+'\"')\n",
    "#     except:\n",
    "#         print(region)\n",
    "#         pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Just Substations (v1) code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# country_code = 'ZW'\n",
    "# update = False # prevents re-download and filteration of file\n",
    "\n",
    "\n",
    "def download_and_filter(country_code, update=False):\n",
    "    country_name = AFRICA_CC[country_code]\n",
    "    element_file_exists = False\n",
    "    # json file for the Data dictionary\n",
    "    # json file for the Elements dictionary is automatically written to 'data/osm/Elements'+filename)\n",
    "    JSON_outputfile = os.path.join(\n",
    "        os.getcwd(), \"data\", \"osm\", country_name + \"_substations.json\"\n",
    "    )\n",
    "\n",
    "    if os.path.exists(JSON_outputfile):\n",
    "        element_file_exists = True\n",
    "\n",
    "    geofabrik_filename = f\"{country_name}-latest.osm.pbf\"  # Filename for geofabrik\n",
    "    geofabrik_url = f\"https://download.geofabrik.de/africa/{geofabrik_filename}\"  # https://download.geofabrik.de/africa/nigeria-latest.osm.pbf\n",
    "\n",
    "    PBF_inputfile = os.path.join(\n",
    "        os.getcwd(), \"data\", \"osm\", \"pbf\", geofabrik_filename\n",
    "    )  # Input filepath\n",
    "\n",
    "    if not os.path.exists(PBF_inputfile) or update:\n",
    "        print(f\"{geofabrik_filename} does not exist, downloading to {PBF_inputfile}\")\n",
    "        os.makedirs(\n",
    "            os.path.dirname(PBF_inputfile), exist_ok=True\n",
    "        )  # create data/osm directory\n",
    "        with requests.get(geofabrik_url, stream=True) as r:\n",
    "            with open(PBF_inputfile, \"wb\") as f:\n",
    "                shutil.copyfileobj(r.raw, f)\n",
    "\n",
    "    # see https://dlr-ve-esy.gitlab.io/esy-osmfilter/filter.html for filter structures\n",
    "\n",
    "    prefilter = {\n",
    "        Node: {\"power\": [\"substation\"]},\n",
    "        Way: {\"power\": [\"substation\"]},\n",
    "        Relation: {\"power\": [\"substation\"]},\n",
    "    }\n",
    "    blackfilter = [\n",
    "        (\"pipeline\", \"substation\"),\n",
    "    ]  # HACKY: due to esy.osmfilter\n",
    "    whitefilter = [\n",
    "        [\n",
    "            (\"power\", \"substation\"),\n",
    "        ],\n",
    "    ]\n",
    "\n",
    "    elementname = f\"{country_code}_substations\"\n",
    "\n",
    "    if update is False and element_file_exists is True:\n",
    "        create_elements = False  # Do not create elements again\n",
    "        new_prefilter_data = False  # Do not pre-filter data again\n",
    "\n",
    "        ###Hacking esy.osmfilter code\n",
    "        Data = osm_info.ReadJason(JSON_outputfile, verbose=\"no\")\n",
    "        DataDict = {\"Data\": Data}\n",
    "        osm_pickle.picklesave(\n",
    "            DataDict,\n",
    "            os.path.realpath(\n",
    "                os.path.join(os.getcwd(), os.path.dirname(JSON_outputfile))\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # print(\"Loading Pickle\")\n",
    "    else:\n",
    "        create_elements = True\n",
    "        new_prefilter_data = True\n",
    "        print(\"Creating  New Elements\")\n",
    "\n",
    "    return run_filter(\n",
    "        elementname,\n",
    "        PBF_inputfile,\n",
    "        JSON_outputfile,\n",
    "        prefilter,\n",
    "        whitefilter,\n",
    "        blackfilter,\n",
    "        NewPreFilterData=new_prefilter_data,\n",
    "        CreateElements=create_elements,\n",
    "        LoadElements=True,\n",
    "        verbose=False,\n",
    "        multiprocess=True,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'download_and_filter' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-18323f6cb32a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf_all\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcountry_code\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mAFRICA_CC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;34m[\u001b[0m\u001b[0mData\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mElements\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_and_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcountry_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0melementname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf'{country_code}_substations'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdf_way\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson_normalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mElements\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melementname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Way\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'download_and_filter' is not defined"
     ]
    }
   ],
   "source": [
    "df_all = pd.DataFrame()\n",
    "for country_code in AFRICA_CC.keys():\n",
    "    [Data, Elements] = download_and_filter(country_code)\n",
    "    elementname = f\"{country_code}_substations\"\n",
    "    df_way = pd.json_normalize(Elements[elementname][\"Way\"].values())\n",
    "    df_node = pd.json_normalize(Elements[elementname][\"Node\"].values())\n",
    "\n",
    "    convert_ways_nodes(df_way)\n",
    "    # Add Type Column\n",
    "    df_node[\"Type\"] = \"Node\"\n",
    "    df_way[\"Type\"] = \"Way\"\n",
    "\n",
    "    df_combined = pd.concat([df_node, df_way], axis=0)\n",
    "    # Add Country Column\n",
    "    df_combined[\"Country\"] = AFRICA_CC[country_code]\n",
    "\n",
    "    df_all = pd.concat([df_all, df_combined])\n",
    "\n",
    "\n",
    "df_all.dropna(\n",
    "    subset=[\"tags.voltage\"], inplace=True\n",
    ")  # Drop any substations with Voltage = N/A\n",
    "df_all.dropna(\n",
    "    thresh=len(df_all) * 0.25, axis=1, how=\"all\", inplace=True\n",
    ")  # Drop Columns with 75% values as N/A\n",
    "# display(df_all)\n",
    "\n",
    "\n",
    "# Generate Files\n",
    "outputfile_partial = os.path.join(os.getcwd(), \"data\", \"africa_all\" + \"_substations.\")\n",
    "\n",
    "df_all.to_csv(outputfile_partial + \"csv\")  # Generate CSV\n",
    "\n",
    "gdf = convert_pd_to_gdf(df_all)\n",
    "gdf.to_file(outputfile_partial + \"geojson\", driver=\"GeoJSON\")  # Generate GeoJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Ways to Nodes\n",
    "def convert_ways_nodes(df_way, Data):\n",
    "    lonlat_column = []\n",
    "    for ref in df_way[\"refs\"]:\n",
    "        lonlats = []\n",
    "        for r in ref:\n",
    "            lonlat = Data[\"Node\"][str(r)][\"lonlat\"]\n",
    "            lonlats.append(lonlat)\n",
    "        lonlats = np.array(lonlats)\n",
    "        lonlat = np.mean(lonlats, axis=0)  # Hacky Apporx Centroid\n",
    "        lonlat_column.append(lonlat)\n",
    "    df_way.drop(\"refs\", axis=1, inplace=True)\n",
    "    df_way.insert(1, \"lonlat\", lonlat_column)\n",
    "\n",
    "\n",
    "# polygon_column=[]\n",
    "# for poly_list in lonlat_column:\n",
    "#     for p in poly_list:\n",
    "#         print(p)\n",
    "\n",
    "#     print(\"p\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pd_to_gdf(df_way):\n",
    "    gdf = gpd.GeoDataFrame(df_way, geometry=[Point(x, y) for x, y in df_way.lonlat])\n",
    "    gdf.drop(columns=[\"lonlat\"], inplace=True)\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_pbf(country_code, update):\n",
    "    country_name = AFRICA_CC[country_code]\n",
    "    geofabrik_filename = f\"{country_name}-latest.osm.pbf\"  # Filename for geofabrik\n",
    "    geofabrik_url = f\"https://download.geofabrik.de/africa/{geofabrik_filename}\"  # https://download.geofabrik.de/africa/nigeria-latest.osm.pbf\n",
    "    PBF_inputfile = os.path.join(\n",
    "        os.getcwd(), \"data\", \"osm\", \"pbf\", geofabrik_filename\n",
    "    )  # Input filepath\n",
    "\n",
    "    if not os.path.exists(PBF_inputfile) or update:\n",
    "        print(f\"{geofabrik_filename} does not exist, downloading to {PBF_inputfile}\")\n",
    "        os.makedirs(\n",
    "            os.path.dirname(PBF_inputfile), exist_ok=True\n",
    "        )  # create data/osm directory\n",
    "        with requests.get(geofabrik_url, stream=True) as r:\n",
    "            with open(PBF_inputfile, \"wb\") as f:\n",
    "                shutil.copyfileobj(r.raw, f)\n",
    "\n",
    "    return PBF_inputfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Substation and Lines (v2) code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_and_filter(country_code, update=False):\n",
    "    PBF_inputfile = download_pbf(country_code, update)\n",
    "\n",
    "    filter_file_exists = False\n",
    "    JSON_outputfile = os.path.join(\n",
    "        os.getcwd(), \"data\", \"osm\", country_code + \"_power.json\"\n",
    "    )\n",
    "\n",
    "    if os.path.exists(JSON_outputfile):\n",
    "        filter_file_exists = True\n",
    "\n",
    "    if update is False and filter_file_exists is True:\n",
    "        create_elements = False  # Do not create elements again\n",
    "        new_prefilter_data = False  # Do not pre-filter data again\n",
    "        ###Hacking esy.osmfilter code to re-create Data.pickle\n",
    "        Data = osm_info.ReadJason(JSON_outputfile, verbose=\"no\")\n",
    "        DataDict = {\"Data\": Data}\n",
    "        osm_pickle.picklesave(\n",
    "            DataDict,\n",
    "            os.path.realpath(\n",
    "                os.path.join(os.getcwd(), os.path.dirname(JSON_outputfile))\n",
    "            ),\n",
    "        )\n",
    "        print(\"Loading Pickle\")\n",
    "    else:\n",
    "        create_elements = True\n",
    "        new_prefilter_data = True\n",
    "        print(\"Creating  New Elements\")\n",
    "\n",
    "    prefilter = {\n",
    "        Node: {\"power\": [\"substation\", \"line\"]},\n",
    "        Way: {\"power\": [\"substation\", \"line\"]},\n",
    "        Relation: {\"power\": [\"substation\", \"line\"]},\n",
    "    }\n",
    "    blackfilter = [\n",
    "        (\"pipeline\", \"substation\"),\n",
    "    ]  # HACKY: due to esy.osmfilter\n",
    "\n",
    "    for feature in [\"substation\", \"line\"]:\n",
    "        whitefilter = [\n",
    "            [\n",
    "                (\"power\", feature),\n",
    "            ],\n",
    "        ]\n",
    "        elementname = f\"{country_code}_{feature}s\"\n",
    "\n",
    "        feature_data = run_filter(\n",
    "            elementname,\n",
    "            PBF_inputfile,\n",
    "            JSON_outputfile,\n",
    "            prefilter,\n",
    "            whitefilter,\n",
    "            blackfilter,\n",
    "            NewPreFilterData=new_prefilter_data,\n",
    "            CreateElements=create_elements,\n",
    "            LoadElements=True,\n",
    "            verbose=False,\n",
    "            multiprocess=True,\n",
    "        )\n",
    "\n",
    "        if feature == \"substation\":\n",
    "            substation_data = feature_data\n",
    "        if feature == \"line\":\n",
    "            line_data = feature_data\n",
    "\n",
    "    return (substation_data, line_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_filtered_data_to_dfs(feature_data, feature):\n",
    "    [Data, Elements] = feature_data\n",
    "    elementname = f\"{country_code}_{feature}s\"\n",
    "    df_way = pd.json_normalize(Elements[elementname][\"Way\"].values())\n",
    "    df_node = pd.json_normalize(Elements[elementname][\"Node\"].values())\n",
    "    return (df_node, df_way, Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_substation_data(substation_data):\n",
    "    df_node, df_way, Data = convert_filtered_data_to_dfs(substation_data, \"substation\")\n",
    "    convert_ways_nodes(df_way, Data)\n",
    "    # Add Type Column\n",
    "    df_node[\"Type\"] = \"Node\"\n",
    "    df_way[\"Type\"] = \"Way\"\n",
    "\n",
    "    df_combined = pd.concat([df_node, df_way], axis=0)\n",
    "    # Add Country Column\n",
    "    df_combined[\"Country\"] = AFRICA_CC[country_code]\n",
    "\n",
    "    return df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_line_data(line_data):\n",
    "    df_node, df_way, Data = convert_filtered_data_to_dfs(line_data, \"line\")\n",
    "    convert_ways_lines(df_way, Data)\n",
    "    # Add Type Column\n",
    "    df_way[\"Type\"] = \"Way\"\n",
    "\n",
    "    # Add Country Column\n",
    "    df_way[\"Country\"] = AFRICA_CC[country_code]\n",
    "    return df_way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ways_lines(df_way, Data):\n",
    "    lonlat_column = []\n",
    "    for ref in df_way[\"refs\"]:  # goes through each row in df_way['refs']\n",
    "        lonlats = []\n",
    "        for (\n",
    "            r\n",
    "        ) in (\n",
    "            ref\n",
    "        ):  # picks each element in ref & replaces ID by coordinate tuple (A multiline consist of several points)\n",
    "            lonlat = Data[\"Node\"][str(r)][\n",
    "                \"lonlat\"\n",
    "            ]  # \"r\" is the ID in Data[\"Node\"], [\"lonlat\"] a list of [x1,y1] (coordinates)\n",
    "            lonlat = tuple(lonlat)\n",
    "            lonlats.append(lonlat)  # a list with tuples\n",
    "        lonlat_column.append(lonlats)  # adding a new list of tuples every row\n",
    "    df_way.drop(\"refs\", axis=1, inplace=True)\n",
    "    df_way.insert(1, \"lonlat\", lonlat_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_pd_to_gdf_lines(df_way):\n",
    "    df_way[\"geometry\"] = df_way[\"lonlat\"].apply(lambda x: LineString(x))\n",
    "    gdf = gpd.GeoDataFrame(df_way, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "    gdf.drop(columns=[\"lonlat\"], inplace=True)\n",
    "    ##voltage filter\n",
    "    # gdf.rename(columns = {'tags.voltage':\"voltage_V\"}, inplace = True)\n",
    "    # ## some transmission lines carry multiple voltages, having voltage_V = 10000;20000  (two lines)\n",
    "    # # The following code keeps only the first information before the semicolon..\n",
    "    # # Needs to be corrected in future, creating two lines with the same bus ID.\n",
    "    # gdf['voltage_V'] = gdf['voltage_V'].str.split(';').str[0]\n",
    "    # gdf['voltage_V'] = gdf['voltage_V'].astype(int)\n",
    "    # gdf = gdf[gdf.voltage_V > 10000]\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n",
      "Loading Pickle\n"
     ]
    }
   ],
   "source": [
    "# test_CC = {\"DZ\": \"algeria\", \"EG\": \"egypt\", \"NG\": \"nigeria\"}\n",
    "df_all_substations = pd.DataFrame()\n",
    "df_all_lines = pd.DataFrame()\n",
    "for country_code in AFRICA_CC.keys():\n",
    "    substation_data, line_data = download_and_filter(country_code)\n",
    "    for feature in [\"line\"]:\n",
    "        if feature == \"substation\":\n",
    "            df_substation = process_substation_data(substation_data)\n",
    "            df_all_substations = pd.concat([df_all_substations, df_substation])\n",
    "        if feature == \"line\":\n",
    "            df_line = process_line_data(line_data)\n",
    "            df_all_lines = pd.concat([df_all_lines, df_line])\n",
    "\n",
    "\n",
    "# df_all_lines.dropna(subset=['tags.voltage'], inplace = True) # Drop any substations with Voltage = N/A\n",
    "df_all_lines.dropna(\n",
    "    thresh=len(df_all_lines) * 0.25, axis=1, how=\"all\", inplace=True\n",
    ")  # Drop Columns with 75% values as N/A\n",
    "# display(df_all_lines)\n",
    "\n",
    "\n",
    "# Generate Files\n",
    "outputfile_partial = os.path.join(os.getcwd(), \"data\", \"africa_all\" + \"_lines.\")\n",
    "\n",
    "df_all_lines.to_csv(outputfile_partial + \"csv\")  # Generate CSV\n",
    "\n",
    "gdf = convert_pd_to_gdf_lines(df_all_lines)\n",
    "gdf.to_file(outputfile_partial + \"geojson\", driver=\"GeoJSON\")  # Generate GeoJson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# country_code = 'MA'\n",
    "# [Data,Elements]=download_and_filter(country_code)\n",
    "# elementname=f'{country_code}_substations'\n",
    "# df_way = pd.json_normalize(Elements[elementname][\"Way\"].values())\n",
    "# df_node = pd.json_normalize(Elements[elementname][\"Node\"].values())\n",
    "\n",
    "\n",
    "# try :\n",
    "#     df_way.dropna(subset=['tags.voltage'], inplace = True) # Drop any substations with Voltage = N/A\n",
    "#     df_node.dropna(subset=['tags.voltage'], inplace = True) # Drop any substations with Voltage = N/A\n",
    "#     df_way.dropna(thresh=len(df_way)*0.25, axis=1, how='all', inplace = True) #Drop Columns with 75% values as N/A\n",
    "#     df_node.dropna(thresh=len(df_node)*0.25, axis=1, how='all', inplace = True) #Drop Columns with 75% values as N/A\n",
    "# except:\n",
    "#     print('Problem with' + country_code)\n",
    "#     pass\n",
    "\n",
    "# # Add Type Column\n",
    "# df_node['Type'] = 'Node'\n",
    "# df_way['Type'] = 'Way'\n",
    "# display(df_node)\n",
    "# convert_ways_nodes(df_way)\n",
    "# display(df_way)\n",
    "\n",
    "# df_combined = pd.concat([df_node, df_way], axis = 0)\n",
    "# df_combined.dropna(thresh=len(df_combined)*0.25, axis=1, how='all', inplace = True) #Drop Columns with 75% values as N/A\n",
    "# display(df_combined)\n",
    "# gdf = convert_pd_to_gdf(df_combined)\n",
    "# display(gdf)"
   ]
  }
 ],
 "metadata": {
  "language_info": {},
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
